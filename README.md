# Analytics Engineering - NBA Data Pipeline

Projeto de analytics engineering para transforma√ß√£o e modelagem de dados da NBA usando dbt (data build tool) e BigQuery, com deploy automatizado no Google Cloud Run.

## üìã Vis√£o Geral

Este projeto implementa um pipeline de dados completo para an√°lise de estat√≠sticas da NBA, incluindo:
- Transforma√ß√£o de dados brutos em modelos anal√≠ticos
- Dimens√µes e fatos para an√°lise de apostas esportivas
- Processamento de estat√≠sticas de jogadores, times e jogos
- An√°lise de les√µes e impactos em performance
- Deploy automatizado no Cloud Run para execu√ß√£o agendada

## üèóÔ∏è Arquitetura

```
Raw Data (GCS) ‚Üí Staging (Views) ‚Üí Intermediate (Views) ‚Üí Marts (Tables)
```

- **Staging**: Limpeza e padroniza√ß√£o dos dados brutos
- **Intermediate**: Transforma√ß√µes intermedi√°rias e l√≥gica de neg√≥cio
- **Marts**: Modelos finais para consumo (dimens√µes e fatos)

## üìÅ Estrutura do Projeto

```
analytics-engineering/
‚îú‚îÄ‚îÄ dbt_nba/                    # Projeto dbt
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ staging/           # Modelos de staging (views)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ intermediate/      # Modelos intermedi√°rios (views)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ marts/             # Modelos finais (tables)
‚îÇ   ‚îú‚îÄ‚îÄ dbt_project.yml        # Configura√ß√£o do projeto dbt
‚îÇ   ‚îî‚îÄ‚îÄ packages.yml           # Depend√™ncias do dbt
‚îú‚îÄ‚îÄ profiles.yml                # Configura√ß√£o de conex√£o BigQuery
‚îú‚îÄ‚îÄ Dockerfile                  # Imagem Docker para Cloud Run
‚îú‚îÄ‚îÄ build-and-push.sh          # Script de build e deploy
‚îú‚îÄ‚îÄ requirements.txt            # Depend√™ncias Python
‚îî‚îÄ‚îÄ README.md                   # Esta documenta√ß√£o
```

## üöÄ Configura√ß√£o Local

### Pr√©-requisitos

- Python 3.13+
- dbt-core e dbt-bigquery instalados
- Acesso ao projeto GCP `smartbetting-dados`
- Autentica√ß√£o gcloud configurada (`gcloud auth application-default login`)

### Instala√ß√£o

1. **Clone o reposit√≥rio** (se aplic√°vel)

2. **Instale as depend√™ncias Python:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Configure o dbt para usar o profiles.yml da raiz:**
   ```bash
   export DBT_PROFILES_DIR=$(pwd)
   ```

   Ou adicione ao seu `~/.zshrc` ou `~/.bashrc`:
   ```bash
   export DBT_PROFILES_DIR=/caminho/absoluto/para/analytics-engineering
   ```

4. **Instale as depend√™ncias do dbt:**
   ```bash
   cd dbt_nba
   dbt deps
   ```

### Executando Localmente

```bash
# Navegar para o diret√≥rio do projeto dbt
cd dbt_nba

# Verificar conex√£o
dbt debug

# Executar todos os modelos
dbt run

# Executar modelos espec√≠ficos
dbt run --select staging.*
dbt run --select marts.*

# Executar testes
dbt test

# Gerar documenta√ß√£o
dbt docs generate
dbt docs serve
```

## ‚òÅÔ∏è Deploy no Cloud Run

### Pr√©-requisitos

- Google Cloud SDK instalado e configurado
- Permiss√µes para criar Cloud Run Jobs
- Artifact Registry configurado
- Service Account com permiss√µes adequadas

### Permiss√µes do Service Account

O service account usado no Cloud Run Job precisa das seguintes roles:

- `roles/bigquery.dataEditor` - Criar/atualizar tabelas e views
- `roles/bigquery.jobUser` - Executar queries no BigQuery
- `roles/bigquery.dataViewer` - Ler dados das tabelas fonte

**Ou use a role mais completa:**
- `roles/bigquery.user` - Inclui todas as permiss√µes acima

### Build e Push da Imagem Docker

1. **Configure as vari√°veis de ambiente (opcional):**
   ```bash
   export GCP_PROJECT_ID=smartbetting-dados
   export GCP_REGION=us-east1
   export ARTIFACT_REGISTRY_REPO=dbt-nba-repo
   export IMAGE_NAME=dbt-nba
   export IMAGE_TAG=latest
   ```

2. **Execute o script de build e push:**
   ```bash
   chmod +x build-and-push.sh
   ./build-and-push.sh
   ```

   O script ir√°:
   - Configurar autentica√ß√£o Docker com gcloud
   - Criar builder buildx para multiplataforma
   - Fazer build da imagem para `linux/amd64` (requerido pelo Cloud Run)
   - Fazer push para o Artifact Registry

### Configura√ß√£o do Cloud Run Job

1. **Crie um Cloud Run Job** apontando para a imagem:
   ```
   us-east1-docker.pkg.dev/smartbetting-dados/dbt-nba-repo/dbt-nba:latest
   ```

2. **Configure o Service Account:**
   - Use um service account com as permiss√µes BigQuery mencionadas acima
   - Configure no Cloud Run Job: `--service-account=SERVICE_ACCOUNT_EMAIL`

3. **Configure o comando (opcional):**
   - O comando padr√£o √©: `dbt run --target prod`
   - Voc√™ pode sobrescrever no Cloud Run Job para executar comandos espec√≠ficos

4. **Agende a execu√ß√£o (opcional):**
   - Use Cloud Scheduler para executar o job periodicamente

## üìä Modelos de Dados

### Staging (`staging/`)

Modelos que fazem limpeza e padroniza√ß√£o dos dados brutos:

- `stg_active_players` - Jogadores ativos da NBA
- `stg_games` - Informa√ß√µes dos jogos
- `stg_game_player_stats` - Estat√≠sticas individuais dos jogadores por jogo
- `stg_player_injuries` - Informa√ß√µes de les√µes
- `stg_player_props` - Props de apostas dos jogadores
- `stg_season_averages_general_base` - M√©dias gerais da temporada
- `stg_season_averages_general_advanced` - Estat√≠sticas avan√ßadas
- `stg_season_averages_shooting_by_zone` - Estat√≠sticas de arremesso por zona
- `stg_team_standings` - Classifica√ß√£o dos times

### Intermediate (`intermediate/`)

Transforma√ß√µes intermedi√°rias e l√≥gica de neg√≥cio:

- `int_game_player_stats_pilled` - Estat√≠sticas de jogos em formato longo
- `int_game_player_stats_not_played` - Jogos onde o jogador n√£o participou
- `int_game_player_stats_last_game_text` - Texto descritivo do √∫ltimo jogo
- `int_games_teams_pilled` - Dados de jogos por perspectiva do time
- `int_season_averages_general_base` - Agrega√ß√µes de m√©dias da temporada

### Marts (`marts/`)

Modelos finais para consumo anal√≠tico:

#### Dimens√µes

- `dim_players` - Dimens√£o completa de jogadores (com les√µes e √∫ltimo jogo)
- `dim_stat_player` - Dimens√£o de estat√≠sticas de jogadores (com ratings e an√°lise de backup)
- `dim_teams` - Dimens√£o de times (com standings e pr√≥ximos jogos)
- `dim_player_shooting_by_zones` - Estat√≠sticas de arremesso por zona

#### Fatos

- `ft_games` - Fato de jogos
- `ft_game_player_stats` - Fato de estat√≠sticas de jogadores por jogo

## üîß Configura√ß√£o

### Profiles.yml

O arquivo `profiles.yml` na raiz do projeto cont√©m as configura√ß√µes de conex√£o:

- **dev**: Ambiente de desenvolvimento (usa OAuth local)
- **prod**: Ambiente de produ√ß√£o (usa OAuth via Application Default Credentials no Cloud Run)

### Vari√°veis de Ambiente

O projeto n√£o requer vari√°veis de ambiente, pois os valores est√£o configurados diretamente no `profiles.yml`. Para desenvolvimento local, certifique-se de ter autentica√ß√£o gcloud configurada.

## üß™ Testes

Execute os testes do dbt:

```bash
cd dbt_nba
dbt test
```

Os testes incluem:
- Valida√ß√£o de unicidade
- Valida√ß√£o de n√£o-nulos
- Testes customizados definidos nos arquivos `models.yml`

## üìö Documenta√ß√£o

### Documenta√ß√£o online (GitHub Pages)

A documenta√ß√£o do dbt (linhagem, descri√ß√µes dos modelos, testes) √© publicada automaticamente no GitHub Pages a cada push em `main`/`master`. Acesse:

**[Ver documenta√ß√£o do dbt](https://tech-lamjav.github.io/analytics-engineering/)**

**Importante:** para o link mostrar os docs do dbt (e n√£o este README), em **Settings > Pages** use **Source** = ‚ÄúDeploy from a branch‚Äù, **Branch** = `gh-pages`, **Folder** = `/ (root)`. Se estiver em `main`/`master`, o site exibir√° o README.

Para catalog completo (metadados de colunas do BigQuery), adicione o secret `BIGQUERY_SA_KEY` em **Settings > Secrets and variables > Actions** com o JSON da service account (somente leitura no projeto/dataset).

### Documenta√ß√£o local

Gere e visualize a documenta√ß√£o localmente:

```bash
cd dbt_nba
dbt docs generate
dbt docs serve
```

Isso abrir√° a documenta√ß√£o interativa no navegador com:
- Linhagem de dados
- Descri√ß√µes dos modelos
- Testes e valida√ß√µes
- Depend√™ncias entre modelos

## üîÑ Workflow de Desenvolvimento

1. **Desenvolvimento Local:**
   - Fa√ßa altera√ß√µes nos modelos SQL
   - Teste localmente com `dbt run --select <modelo>`
   - Execute testes com `dbt test`

2. **Commit e Push:**
   - Commit suas altera√ß√µes
   - Push para o reposit√≥rio

3. **Deploy:**
   - Execute `./build-and-push.sh` para build e push da imagem
   - A imagem ser√° atualizada no Artifact Registry
   - O Cloud Run Job usar√° a nova imagem na pr√≥xima execu√ß√£o

## üêõ Troubleshooting

### Erro de autentica√ß√£o local

```bash
gcloud auth application-default login
```

### dbt n√£o encontra o profiles.yml

Certifique-se de ter configurado:
```bash
export DBT_PROFILES_DIR=$(pwd)
```

### Erro de plataforma no Cloud Run

O script `build-and-push.sh` j√° configura o build para `linux/amd64`. Se ainda houver problemas, verifique se o buildx est√° configurado corretamente.

### Permiss√µes no BigQuery

Verifique se o service account tem as roles necess√°rias:
- `roles/bigquery.user` (ou as roles espec√≠ficas mencionadas acima)

## üìù Recursos Adicionais

- [Documenta√ß√£o dbt](https://docs.getdbt.com/docs/introduction)
- [dbt BigQuery Setup](https://docs.getdbt.com/docs/core/connect-data-platform/bigquery-setup)
- [Cloud Run Jobs](https://cloud.google.com/run/docs/create-jobs)
- [BigQuery IAM Roles](https://cloud.google.com/bigquery/docs/access-control)

## üìÑ Licen√ßa

[Adicione informa√ß√µes de licen√ßa se aplic√°vel]

## üë• Contribuidores

[Adicione informa√ß√µes de contribuidores se aplic√°vel]
